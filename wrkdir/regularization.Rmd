---
title: "Hands on with Regularization and Tree based Model Ensemble"
output: html_notebook
---


```{r}
housing_df = read.csv("housing.csv")[,-1]
## first column is Id so it can be removed
```

## Section 1. Data Cleaning

__1.__ Take a summary of the data and explore the result. How many categorical and numerical variables are
there in the dataset?

```{r}
summary(housing_df)
```
```{r}
length(housing_df[,sapply(housing_df, is.character) == TRUE]) ## Number of categorical variables
length(housing_df[,sapply(housing_df, is.numeric) == TRUE]) ## Number of numerical variables
```

__2.__ (1pt) Which columns have missing values and what percentage of those columns have NAs? (Note.
You can use colMeans(is.na(your data frame)) to find the percentage of NAs in each column).
```{r}
missing_vals_columns = which(colSums(is.na(housing_df)) > 0)
sort(colSums(sapply(housing_df[missing_vals_columns], is.na)), decreasing = TRUE)
```
The above columns have missing values
```{r}
means = colMeans(sapply(housing_df[missing_vals_columns], is.na))
newmeans = as.numeric(sprintf("%2.3f", means)) * 100
names(newmeans) = names(means)
sort(newmeans, decreasing = TRUE)
```
The above are the % NAs in each column with NAs

__3.__ Is there any obvious outlier in the SalePrice? If so, remove them
```{r}
## Before transformation
str(housing_df$SalePrice)
first_quant = quantile(housing_df$SalePrice)[2]
third_quant = quantile(housing_df$SalePrice)[4]
iqr = third_quant - first_quant
iqr_index = (housing_df$SalePrice > first_quant - 1.5*iqr & housing_df$SalePrice < third_quant + 1.5*iqr)
housing_df = housing_df[iqr_index, ]
## After transformation
str(housing_df$SalePrice)
```

__4.__ (2pt)Read the data description carefully. For some of the variables, such as PoolQC, FirePlaceQU,
Fence, etc. NA means not applicable rather than missing at random. For instance, a house that does
not have a pool gets NA for PoolQC. For those variables for which NA means not applicable, you
can replace NA with zero ( if that variable is numeric) or replace it with a new category/level, for
instance, “notApplicable” if that variable is categorical.

```{r}
# Cleaning categorical variables
cat_columns = c("Alley","BsmtQual","BsmtCond","BsmtExposure","BsmtFinType1","BsmtFinType2","FireplaceQu","GarageType","GarageFinish","GarageQual","GarageCond","PoolQC","Fence","MiscFeature")
new_columns = lapply(cat_columns, function(x){
  housing_df[is.na(housing_df[,x]),x] <<- "notApplicable"
})

# Cleaning numerical variables
num_columns = (sapply(housing_df, is.numeric) == TRUE) & (colSums(is.na(housing_df)) > 0)
new_columns = lapply(names(housing_df[,num_columns]), function(x){
  housing_df[is.na(housing_df[,x]),x] <<- 0
})
```

__5.__ (1pt) After replacing not applicable NAs with appropriate values, find out which columns still
have NAs and what percentage of each column is missing.

```{r}
missing_vals_columns = which(colSums(is.na(housing_df)) > 0)
means = colMeans(sapply(housing_df[missing_vals_columns], is.na))
newmeans = as.numeric(sprintf("%2.3f", means)) * 100
names(newmeans) = names(means)
sort(newmeans, decreasing = TRUE)
```

__6.__ (1pt) what percentage of rows in the dataset have one or more missing values? Use
“complete.cases” function to answer this question.

```{r}
missing_count = length(which(!complete.cases(housing_df)))
as.numeric(sprintf("%2.3f",missing_count/1399)) * 100 # 1399 is the number of observations in the data frame
```
0.6% of rows have missing values

## Section 2. Data Exploration

__8.__ (1pt) plot the histogram of SalePrice. Interpret the histogram. Is SalePrice variable skewed?
To replace SalePrice with log(SalePrice. Compare the histogram of salesprice before and after log
transformation.

```{r}
hist(housing_df$SalePrice, xlab = "SalePrice", main = "Sale Price")
```
Yes, the SalePrice variable is right skewed

```{r}
hist(log(housing_df$SalePrice), xlab = "Log SalePrice", main = "Sale Price")
```

The histogram of Log SalePrice is left skewed.

__9.__ (2 pt) Use plot (SalePrice~. , data=housing) (replace housing with your dataframe after data
cleaning) to draw scatter and side by side box plots of other variables against the Sale Price. From these
plots, what variables seem to have correlation with SalePrice? (Note since we have so many variables,
you do not need to use statistics tests, you can just answer this question based on your observations of
the plots)

```{r}
attach(housing_df)
```

The plots for categorical variables
```{r}
colName = names(housing_df)
correlated_variables = vector(mode = "character")
temp = lapply(colName, function(col) {
  if (is.character(housing_df[,col])) {
      chisq_temp = chisq.test(SalePrice, as.factor(housing_df[,col]))
      if (chisq_temp$p.value < 0.006) {
        correlated_variables <<- append(correlated_variables, col)
      }
      plot(SalePrice ~ as.factor(housing_df[,col]), xlab = col)
  }
  })

non_scatter = c("MSSubClass","OverallQual","OverallCond","BsmtFullBath","BsmtFullBath","FullBath","HalfBath","BedroomAbvGr","KitchenAbvGr","TotRmsAbvGrd","Fireplaces","GarageCars","MoSold","YrSold","")

temp1 = lapply(non_scatter, function(col) {
      chisq_temp = chisq.test(SalePrice, as.factor(housing_df[,col]))
      if (chisq_temp$p.value < 0.006) {
        correlated_variables <<- append(correlated_variables, col)
      }
      plot(SalePrice ~ as.factor(housing_df[,col]), xlab = col)
})
```
The scatter plots for numeric variables
```{r}
col_name = names(housing_df)

scatter_cols = setdiff(col_name, non_scatter)

temp = lapply(scatter_cols, function(col) {
  if (is.numeric(housing_df[,col]) & col != "SalePrice") {
      cor_temp = cor.test(SalePrice, housing_df[,col])
      if (cor_temp$estimate >= 0.5 | cor_temp$estimate <= -0.5) {
        correlated_variables <<- append(correlated_variables, col)
      }
      plot(SalePrice ~ housing_df[,col], xlab = col)
  }
  return()
  })

```
Below are the variables I've interpreted to be correlated with SalePrice
```{r}
correlated_variables
```

__10.__ (2 pt) Examine the columns with missing values to see if any of them are categorical.
Use caret’s createDataPartition method to partition the dataset to 80% training and 20% testing. If a categorical column has missing values in train or test data, impute it with the mode of that column in the training data. It is important that the mode is computed based only on the training data only (instead of the entire dataset) to avoid data leakage.